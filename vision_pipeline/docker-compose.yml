services:
  ollama:
    image: ollama/ollama:latest
    container_name: vision-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./shared:/shared
    environment:
      - TZ=Europe/Warsaw
      - OLLAMA_HOST=0.0.0.0:11434
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Uncomment below and comment out deploy section above for CPU-only mode
    # environment:
    #   - OLLAMA_NUM_GPU=0

  sam-service:
    build:
      context: ./sam-service
      dockerfile: Dockerfile
    container_name: vision-sam
    ports:
      - "8001:8001"
    volumes:
      - sam-models:/models
      - ./shared:/shared
    environment:
      - TZ=Europe/Warsaw
      - SAM_MODEL_TYPE=vit_h
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import requests; requests.get(\"http://localhost:8001/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Uncomment below and comment out deploy section above for CPU-only mode
    # environment:
    #   - TZ=Europe/Warsaw
    #   - SAM_MODEL_TYPE=vit_h
    #   - FORCE_CPU=true

  pipeline-api:
    build:
      context: ./pipeline-api
      dockerfile: Dockerfile
    container_name: vision-pipeline
    ports:
      - "8080:8080"
    volumes:
      - ./shared:/shared
    environment:
      - TZ=Europe/Warsaw
      - OLLAMA_URL=http://ollama:11434
      - SAM_URL=http://sam-service:8001
      - DEFAULT_MODEL=qwen2-vl:72b
    depends_on:
      ollama:
        condition: service_started
      sam-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import requests; requests.get(\"http://localhost:8080/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  rembg-service:
    build:
      context: ./rembg-service
      dockerfile: Dockerfile
    container_name: vision-rembg
    ports:
      - "8002:8002"
    volumes:
      - rembg-models:/root/.u2net
      - ./shared:/shared
    environment:
      - TZ=Europe/Warsaw
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import requests; requests.get(\"http://localhost:8002/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Uncomment below and comment out deploy section above for CPU-only mode
    # environment:
    #   - TZ=Europe/Warsaw
    #   - FORCE_CPU=true

volumes:
  ollama-data:
    driver: local
  sam-models:
    driver: local
  rembg-models:
    driver: local

networks:
  default:
    name: vision-network
